{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch4_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_JlDa3HjkRA"
      },
      "source": [
        "import numpy as np\n",
        "def numerical_diff(f,w):\n",
        "  h=1e-4\n",
        "  return (f(w+h)-f(w-h))/(2*h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2_sWjBok2cJ"
      },
      "source": [
        "def function_tmp1(w0):\n",
        "  return w0*w0+2.0**2\n",
        "def function_tmp2(w1):\n",
        "  return 1.0**2 +w1*w1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcYcF--blWzk",
        "outputId": "a0e3af4d-1c0c-4954-ab21-fbf1d6988c90"
      },
      "source": [
        "print(\"w0에 대한 편미분\",numerical_diff(function_tmp1, 1.0))# w0에 대한 편미분\n",
        "print(\"w1에 대한 편미분\",numerical_diff(function_tmp2, 2.0)) # w1에 대한 편미분"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w0에 대한 편미분 1.9999999999997797\n",
            "w1에 대한 편미분 4.000000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5HKLkBGxi4o",
        "outputId": "cecccf28-7ff0-4d89-9817-c469ff4df605"
      },
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch.git # 한국인 저자의 밑바닥딥러닝 깃허브 클론해오기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch'...\n",
            "remote: Enumerating objects: 826, done.\u001b[K\n",
            "remote: Total 826 (delta 0), reused 0 (delta 0), pack-reused 826\u001b[K\n",
            "Receiving objects: 100% (826/826), 52.21 MiB | 22.60 MiB/s, done.\n",
            "Resolving deltas: 100% (477/477), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIoVncSgx3ou",
        "outputId": "e0117a2d-6f82-42c1-f03a-087e9b57b42d"
      },
      "source": [
        "!pwd "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HToCeXryyCe6",
        "outputId": "63025441-6db3-4e85-822c-a0ad6fab86b4"
      },
      "source": [
        "cd deep-learning-from-scratch/ch04 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch/ch04/deep-learning-from-scratch/ch04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDN0Ksbh1GoI"
      },
      "source": [
        "## 4.5 학습알고리즘 구현하기\n",
        "\n",
        "신경망 학습의 순서를 확인해보자.\n",
        "\n",
        "### 전체  \n",
        " 신경망에는 적용 가능한 가중치와 편향이 있고, 이 가중치(weight)와 편향(bias)을 훈련 데이터(training data)에 적용하도록 조정하는 과정을 학습이라고 한다.  \n",
        "  신경망은 4단계를 거쳐서 학습이 된다.\n",
        "\n",
        "1.    __미니배치(mini batch):__ 훈련데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라고 하고 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n",
        "2.   __기울기 산출__:미니배치의 손실함수 값을 줄이기 위해서 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실함수의 값을 가장 작게하는 방향을 제시한다.\n",
        "\n",
        "3.  __매개변수 갱신__: 가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다.\n",
        "\n",
        "4. __반복__: 1~3단계를 반복한다.\n",
        "\n",
        "\n",
        "\n",
        "데이터를 미니배치로 무작위로 선정하기 때문에 __확률적 경사 하강법(SGD,Stochastic Gradient Descent)__라고 부른다. \n",
        "\n",
        "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch4_5.jpg?raw=true)\n",
        "\n",
        "[출처: Mini batch SGD가 무슨 의미일까요?](https://youtu.be/w_hugN_fv2M)\n",
        "![](https://github.com/Asunny2019/Deeplearning_from_scratch/blob/main/Deeplearning_from_scratch1/img/ch4_6_SGD_2.jpg?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "yR61DVcDAVKe",
        "outputId": "a9444295-fd06-4647-952f-0d40101a89e7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.random.seed(10) # 랜덤함수의 seed\n",
        "a= 0.1 * np.random.randn(1000)\n",
        "b= 0.01 * np.random.randn(1000)\n",
        "\n",
        "plt.hist(a)\n",
        "plt.hist(b)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rX1IpODAyMOF"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "        # 가중치와 편향을 초기화한다. input_size: 데이터의 feature, \n",
        "        #hidden_size: hidden layer의 개수(일단은 임의의 값, 나중에는 학습이 잘 되게 하기 위해 찾아내야하는 값), \n",
        "        #output_size: 출력 할때 클래스 개수(mnist의 경우 10개), \n",
        "        # weight_init_std는 W1과 W2의 np.random.randn함수를 이해할 필요가 있다. \n",
        "        # 이 함수는 정규분포를 띄는 값을 계산해준다.\n",
        "        #https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #레이어1 의 가중치\n",
        "        self.params['b1'] = np.zeros(hidden_size) # 레이어 1의 편향(bias)0\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)#레이어2 의 가중치\n",
        "        self.params['b2'] = np.zeros(output_size) # 레이어 2의 편향(bias)\n",
        "\n",
        "    def predict(self, x): # 예측 수행\n",
        "        W1, W2 = self.params['W1'], self.params['W2'] # 레이어 1,2의 가중치\n",
        "        b1, b2 = self.params['b1'], self.params['b2'] # 레이어 1,2의 편향\n",
        "    \n",
        "        a1 = np.dot(x, W1) + b1 # 레이어1의 가중치와 편향으로 계산한 값 a1 \n",
        "        z1 = sigmoid(a1) # a1을 시그모이드- 활성화함수(activation function)을 적용한 값\n",
        "        a2 = np.dot(z1, W2) + b2 #  z1의 결과값을 입력데이터로 받아와서 레이어2에서 w2와 계산한다.\n",
        "        y = softmax(a2) # a2를 소프트맥스-활성화함수를 적용한 값\n",
        "        \n",
        "        return y\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)  # 입력 데이터x를 통해 예측한 값 y\n",
        "        \n",
        "        return cross_entropy_error(y, t)\n",
        "        # y와 정답 레이블 t(ground truth)를 손실함수 cross entropy 사용하여\n",
        "        # 둘 사이의 에러가 얼마나 되는지 비교한다.\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        t = np.argmax(t, axis=1)\n",
        "        # softmax를 거쳐서 예측한 값 y에서 가장 큰 값이 있는 클래스가 정답일 가능성이 가장 높으므로 해당 인덱스를 정답으로 간주하여서\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0]) # 비교한다.\n",
        "        return accuracy\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t): # 가중치 매개변수의 기울기를 구한다.\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "        grads = {}\n",
        "        \n",
        "        batch_num = x.shape[0]\n",
        "        \n",
        "        # forward\n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        # backward\n",
        "        dy = (y - t) / batch_num\n",
        "        grads['W2'] = np.dot(z1.T, dy)\n",
        "        grads['b2'] = np.sum(dy, axis=0)\n",
        "        \n",
        "        da1 = np.dot(dy, W2.T)\n",
        "        dz1 = sigmoid_grad(a1) * da1\n",
        "        grads['W1'] = np.dot(x.T, dz1)\n",
        "        grads['b1'] = np.sum(dz1, axis=0)\n",
        "\n",
        "        return grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGZxIORnFgWw"
      },
      "source": [
        "TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 갖습니다. params 변수에는 가중치 매개변수가 저장된다. params 변수에는 이 신경망에 필요한 매개변수가 모두 저장된다. 그리고 params 변수에 저장된 가중치 매개 변수가 순방향으로 예측 처리에서 사용된다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw0n8uV5yfRu",
        "outputId": "819d35ab-a78f-4923-a638-28f05c84da48"
      },
      "source": [
        "net=TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
        "print(net.params['W1'].shape)\n",
        "print(net.params['b1'].shape)\n",
        "print(net.params['W2'].shape)\n",
        "print(net.params['b2'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784, 100)\n",
            "(100,)\n",
            "(100, 10)\n",
            "(10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMwNOap1yhZT",
        "outputId": "ee0a2f33-c02c-4b57-f56c-3775f7db1342"
      },
      "source": [
        "x=np.random.rand(100,784)# 더미 입력데이터 100장분량\n",
        "t=np.random.rand(100,10) # 더미 정답데이터 100장분량\n",
        "grads=net.numerical_gradient(x,t) \n",
        "#대략 3분 정도 걸림\n",
        "# 각 매개변수의 기울기를 계산한다. 수치미분으로 각 매개변수의 기울기를 계산한다.\n",
        "print(grads['W1'].shape)\n",
        "print(grads['b1'].shape)\n",
        "print(grads['W2'].shape)\n",
        "print(grads['b2'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(784, 100)\n",
            "(100,)\n",
            "(100, 10)\n",
            "(10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr96Ax4TIFED"
      },
      "source": [
        "## 4.5.2미니배치 학습구현하기 \n",
        "\n",
        "신경망 학습 구현에는 앞에서 설명한 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "svLJvSxcyhQe",
        "outputId": "83d6e708-6b6b-4f75-daea-11cde54490c9"
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "# 하이퍼파라미터\n",
        "iters_num = 100  # 반복 횟수를 적절히 설정한다.\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100   # 미니배치 크기\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# 1에폭당 반복 수\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    # 미니배치 획득\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # 매개변수 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    # 학습 경과 기록\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    # 1에폭당 정확도 계산\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-80fd6b44fc46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtwo_layer_net\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwoLayerNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVtMZVlBJQAC"
      },
      "source": [
        "매번 60000개의 훈련데이터에서 임의로 100개의 데이터(이미지 데이터와 정답 레이브 데이터)를 추려냅니다. 그리고 100개의 미니배치를 대상으로 확률적 경사하강법을 수행하여 매개변수(parameter)를 갱신한다. 경사하강법에 의한 갱신횟수를 10000번으로 설정하고 갱신할때마다 훈련데이터에 대한 손실함수를 계산하고 그 값을 배열에 추가한다. 그 손실함수의 변화의 추이를 그래프로 나타낸다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NClaDidHKyJK"
      },
      "source": [
        "4.5.3 시험데이터로 평가하기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ug-HX7iyhG2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m56o_pMGyg6v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tqy68-lygsr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS3i90OwygOI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdMPV-GHyfv2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}